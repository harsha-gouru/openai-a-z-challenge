{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Deep Insights: Initial Data Exploration\n",
    "\n",
    "This notebook provides an initial exploration of LiDAR and ancillary datasets for the Amazon rainforest. We'll download sample data, visualize LiDAR point clouds, generate basic terrain models, and explore potential archaeological and ecological features.\n",
    "\n",
    "## Contents\n",
    "1. Setup Environment\n",
    "2. Download Sample Data\n",
    "3. LiDAR Data Visualization\n",
    "4. Basic Terrain Analysis\n",
    "5. Feature Detection Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Geospatial libraries\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.plot import show as rshow\n",
    "import folium\n",
    "import pyproj\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "\n",
    "# LiDAR specific\n",
    "import laspy\n",
    "import pdal\n",
    "import json\n",
    "\n",
    "# Visualization\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add project root to path to import project modules\n",
    "project_root = Path(os.getcwd()).parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Configure paths\n",
    "RAW_DATA_DIR = os.environ.get('RAW_DATA_DIR', os.path.join(project_root, 'data', 'raw'))\n",
    "PROCESSED_DATA_DIR = os.environ.get('PROCESSED_DATA_DIR', os.path.join(project_root, 'data', 'processed'))\n",
    "LIDAR_TILES_DIR = os.environ.get('LIDAR_TILES_DIR', os.path.join(RAW_DATA_DIR, 'lidar'))\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(LIDAR_TILES_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Raw data directory: {RAW_DATA_DIR}\")\n",
    "print(f\"Processed data directory: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Sample Data\n",
    "\n",
    "Let's download a small sample of LiDAR data from the ORNL DAAC repository. We'll use a sample from the \"LiDAR Forest Inventory in Brazil\" dataset (ds_id=1644)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def download_file(url, local_path):\n",
    "    \"\"\"Download a file with progress bar\"\"\"\n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"File already exists at {local_path}\")\n",
    "        return local_path\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    \n",
    "    # Stream download with progress bar\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(local_path, 'wb') as file, tqdm(\n",
    "        desc=os.path.basename(local_path),\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "    \n",
    "    return local_path\n",
    "\n",
    "# For this example, we'll use a small sample LiDAR file\n",
    "# Note: In a real implementation, we would fetch from the actual ORNL DAAC API\n",
    "# but for demo purposes, we'll use a publicly accessible LiDAR sample\n",
    "\n",
    "# Sample LiDAR file URL (this is a placeholder - replace with actual Amazon data URL)\n",
    "sample_lidar_url = \"https://github.com/PDAL/data/raw/master/autzen/autzen-classified.las\"\n",
    "sample_lidar_path = os.path.join(LIDAR_TILES_DIR, \"sample\", \"sample_classified.las\")\n",
    "\n",
    "# Download the sample file\n",
    "try:\n",
    "    downloaded_file = download_file(sample_lidar_url, sample_lidar_path)\n",
    "    print(f\"Downloaded sample LiDAR file to: {downloaded_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading file: {e}\")\n",
    "    # If download fails, check if we have any LAS/LAZ files in the directory\n",
    "    existing_files = list(Path(LIDAR_TILES_DIR).glob(\"**/*.la[sz]\"))\n",
    "    if existing_files:\n",
    "        sample_lidar_path = str(existing_files[0])\n",
    "        print(f\"Using existing LiDAR file: {sample_lidar_path}\")\n",
    "    else:\n",
    "        print(\"No LiDAR files found. Please download manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Supplementary Data\n",
    "\n",
    "Let's also download a sample of known archaeological sites in the Amazon region to provide context for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple GeoJSON with known archaeological sites\n",
    "# In a real implementation, we would fetch this from a proper source\n",
    "# This is just placeholder data for demonstration\n",
    "\n",
    "known_sites = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\n",
    "                \"name\": \"Geoglyph Site 1\",\n",
    "                \"type\": \"Geometric earthwork\",\n",
    "                \"discovered\": 2009\n",
    "            },\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Point\",\n",
    "                \"coordinates\": [-67.4, -9.8]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\n",
    "                \"name\": \"Geoglyph Site 2\",\n",
    "                \"type\": \"Circular earthwork\",\n",
    "                \"discovered\": 2016\n",
    "            },\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Point\",\n",
    "                \"coordinates\": [-67.5, -9.9]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\n",
    "                \"name\": \"Terra Preta Site\",\n",
    "                \"type\": \"Dark earth settlement\",\n",
    "                \"discovered\": 2005\n",
    "            },\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Point\",\n",
    "                \"coordinates\": [-62.2, -3.4]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save to GeoJSON file\n",
    "sites_dir = os.path.join(RAW_DATA_DIR, \"vectors\", \"known_sites\")\n",
    "os.makedirs(sites_dir, exist_ok=True)\n",
    "sites_path = os.path.join(sites_dir, \"sample_sites.geojson\")\n",
    "\n",
    "with open(sites_path, 'w') as f:\n",
    "    json.dump(known_sites, f)\n",
    "\n",
    "print(f\"Created sample archaeological sites GeoJSON at: {sites_path}\")\n",
    "\n",
    "# Load as GeoDataFrame for later use\n",
    "sites_gdf = gpd.read_file(sites_path)\n",
    "sites_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LiDAR Data Visualization\n",
    "\n",
    "Now that we have our sample data, let's explore the LiDAR point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LiDAR file using laspy\n",
    "try:\n",
    "    las = laspy.read(sample_lidar_path)\n",
    "    print(f\"LiDAR file loaded: {sample_lidar_path}\")\n",
    "    print(f\"Number of points: {len(las.points)}\")\n",
    "    print(f\"Point format: {las.header.point_format}\")\n",
    "    print(f\"CRS: {las.header.parse_crs()}\")\n",
    "    \n",
    "    # Print available dimensions\n",
    "    print(\"\\nAvailable dimensions:\")\n",
    "    for dim in las.point_format.dimensions:\n",
    "        print(f\"  - {dim.name}\")\n",
    "        \n",
    "    # Print classification counts if available\n",
    "    if hasattr(las, 'classification'):\n",
    "        class_counts = pd.Series(las.classification).value_counts()\n",
    "        print(\"\\nClassification counts:\")\n",
    "        for cls, count in class_counts.items():\n",
    "            # Standard LAS classification codes\n",
    "            class_names = {\n",
    "                0: \"Created, never classified\",\n",
    "                1: \"Unclassified\",\n",
    "                2: \"Ground\",\n",
    "                3: \"Low Vegetation\",\n",
    "                4: \"Medium Vegetation\",\n",
    "                5: \"High Vegetation\",\n",
    "                6: \"Building\",\n",
    "                7: \"Low Point (noise)\",\n",
    "                8: \"Model Key-point\",\n",
    "                9: \"Water\",\n",
    "                10: \"Rail\",\n",
    "                11: \"Road Surface\",\n",
    "                12: \"Reserved\",\n",
    "                13: \"Wire - Guard (Shield)\",\n",
    "                14: \"Wire - Conductor (Phase)\",\n",
    "                15: \"Transmission Tower\",\n",
    "                16: \"Wire-structure Connector\",\n",
    "                17: \"Bridge Deck\",\n",
    "                18: \"High Noise\"\n",
    "            }\n",
    "            class_name = class_names.get(cls, \"Unknown\")\n",
    "            print(f\"  - Class {cls} ({class_name}): {count} points ({count/len(las.points)*100:.2f}%)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LiDAR file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Visualization of the Point Cloud\n",
    "\n",
    "Let's create a 3D visualization of the point cloud using a sample of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample points to make visualization manageable\n",
    "try:\n",
    "    # Take a random sample of points (adjust sample_size as needed)\n",
    "    sample_size = min(1000000, len(las.points))\n",
    "    indices = np.random.choice(len(las.points), sample_size, replace=False)\n",
    "    \n",
    "    # Extract coordinates\n",
    "    x = las.x[indices]\n",
    "    y = las.y[indices]\n",
    "    z = las.z[indices]\n",
    "    \n",
    "    # Normalize coordinates for better visualization\n",
    "    x_norm = x - np.min(x)\n",
    "    y_norm = y - np.min(y)\n",
    "    z_norm = z - np.min(z)\n",
    "    \n",
    "    # Create color array based on classification if available\n",
    "    if hasattr(las, 'classification'):\n",
    "        # Use classification for coloring\n",
    "        classifications = las.classification[indices]\n",
    "        \n",
    "        # Create a colormap for different classes\n",
    "        cmap = plt.cm.get_cmap('viridis', len(np.unique(classifications)))\n",
    "        colors = cmap(classifications / np.max(classifications))\n",
    "    else:\n",
    "        # Use height (z) for coloring if classification not available\n",
    "        colors = plt.cm.viridis(z_norm / np.max(z_norm))\n",
    "    \n",
    "    # Create 3D scatter plot\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot points\n",
    "    scatter = ax.scatter(x_norm, y_norm, z_norm, c=colors, s=0.1, alpha=0.5)\n",
    "    \n",
    "    # Add colorbar if using classification\n",
    "    if hasattr(las, 'classification'):\n",
    "        cbar = plt.colorbar(scatter)\n",
    "        cbar.set_label('Classification')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X (m)')\n",
    "    ax.set_ylabel('Y (m)')\n",
    "    ax.set_zlabel('Z (m)')\n",
    "    ax.set_title('LiDAR Point Cloud Visualization (Sample)')\n",
    "    \n",
    "    # Improve 3D viewing angle\n",
    "    ax.view_init(elev=20, azim=225)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Add a note about plas.io for better visualization\n",
    "    print(\"Note: For better interactive visualization of large point clouds, consider using plas.io\")\n",
    "    print(\"      or other specialized LiDAR viewers mentioned in the project links.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating 3D visualization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Cloud Profile View\n",
    "\n",
    "Let's create a profile view (cross-section) of the point cloud to better visualize the terrain and vegetation structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a profile view (cross-section) of the point cloud\n",
    "try:\n",
    "    # Define a slice through the middle of the point cloud\n",
    "    y_mid = (np.min(y) + np.max(y)) / 2\n",
    "    slice_width = (np.max(y) - np.min(y)) * 0.01  # 1% of total width\n",
    "    \n",
    "    # Select points within the slice\n",
    "    slice_mask = (las.y > y_mid - slice_width) & (las.y < y_mid + slice_width)\n",
    "    slice_x = las.x[slice_mask]\n",
    "    slice_z = las.z[slice_mask]\n",
    "    \n",
    "    # If available, get classification for coloring\n",
    "    if hasattr(las, 'classification'):\n",
    "        slice_class = las.classification[slice_mask]\n",
    "        \n",
    "        # Create color map\n",
    "        class_colors = {\n",
    "            0: 'gray',      # Created, never classified\n",
    "            1: 'lightgray', # Unclassified\n",
    "            2: 'brown',     # Ground\n",
    "            3: 'yellowgreen', # Low Vegetation\n",
    "            4: 'green',     # Medium Vegetation\n",
    "            5: 'darkgreen', # High Vegetation\n",
    "            6: 'red',       # Building\n",
    "            7: 'black',     # Low Point (noise)\n",
    "            8: 'orange',    # Model Key-point\n",
    "            9: 'blue',      # Water\n",
    "        }\n",
    "        \n",
    "        # Default color for classes not in the map\n",
    "        colors = np.array(['gray'] * len(slice_class))\n",
    "        \n",
    "        # Assign colors based on classification\n",
    "        for cls, color in class_colors.items():\n",
    "            colors[slice_class == cls] = color\n",
    "    else:\n",
    "        # Use height for coloring if classification not available\n",
    "        colors = plt.cm.viridis((slice_z - np.min(slice_z)) / (np.max(slice_z) - np.min(slice_z)))\n",
    "    \n",
    "    # Create the profile plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot points\n",
    "    if hasattr(las, 'classification'):\n",
    "        # Plot by classification groups for better legend\n",
    "        for cls, color in class_colors.items():\n",
    "            mask = slice_class == cls\n",
    "            if np.any(mask):\n",
    "                plt.scatter(slice_x[mask], slice_z[mask], c=color, s=1, alpha=0.7, \n",
    "                           label=f\"Class {cls}\")\n",
    "        plt.legend(markerscale=5)\n",
    "    else:\n",
    "        plt.scatter(slice_x, slice_z, c=colors, s=1, alpha=0.7)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('X (m)')\n",
    "    plt.ylabel('Z (m) - Elevation')\n",
    "    plt.title(f'LiDAR Point Cloud Profile (Y = {y_mid:.2f} ± {slice_width:.2f} m)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error creating profile view: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Terrain Analysis\n",
    "\n",
    "Now let's use PDAL to generate a Digital Elevation Model (DEM) and a Canopy Height Model (CHM) from our LiDAR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "dem_dir = os.path.join(PROCESSED_DATA_DIR, \"dem\")\n",
    "chm_dir = os.path.join(PROCESSED_DATA_DIR, \"chm\")\n",
    "os.makedirs(dem_dir, exist_ok=True)\n",
    "os.makedirs(chm_dir, exist_ok=True)\n",
    "\n",
    "# Output file paths\n",
    "dem_path = os.path.join(dem_dir, \"sample_dem.tif\")\n",
    "chm_path = os.path.join(chm_dir, \"sample_chm.tif\")\n",
    "\n",
    "# Define PDAL pipeline for DEM generation\n",
    "dem_pipeline = {\n",
    "    \"pipeline\": [\n",
    "        sample_lidar_path,\n",
    "        {\n",
    "            \"type\": \"filters.assign\",\n",
    "            \"assignment\": \"Classification[:]=0\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"filters.elm\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"filters.outlier\",\n",
    "            \"method\": \"statistical\",\n",
    "            \"mean_k\": 8,\n",
    "            \"multiplier\": 3.0\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"filters.pmf\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"filters.range\",\n",
    "            \"limits\": \"Classification[2:2]\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"writers.gdal\",\n",
    "            \"filename\": dem_path,\n",
    "            \"output_type\": \"idw\",\n",
    "            \"resolution\": 1.0,\n",
    "            \"window_size\": 10\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Execute the DEM pipeline\n",
    "try:\n",
    "    pipeline = pdal.Pipeline(json.dumps(dem_pipeline))\n",
    "    pipeline.execute()\n",
    "    print(f\"DEM created successfully: {dem_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating DEM: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PDAL pipeline for DSM (Digital Surface Model) generation\n",
    "dsm_path = os.path.join(PROCESSED_DATA_DIR, \"dsm\", \"sample_dsm.tif\")\n",
    "os.makedirs(os.path.dirname(dsm_path), exist_ok=True)\n",
    "\n",
    "dsm_pipeline = {\n",
    "    \"pipeline\": [\n",
    "        sample_lidar_path,\n",
    "        {\n",
    "            \"type\": \"filters.outlier\",\n",
    "            \"method\": \"statistical\",\n",
    "            \"mean_k\": 8,\n",
    "            \"multiplier\": 3.0\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"writers.gdal\",\n",
    "            \"filename\": dsm_path,\n",
    "            \"output_type\": \"max\",  # Use max elevation for DSM\n",
    "            \"resolution\": 1.0,\n",
    "            \"window_size\": 10\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Execute the DSM pipeline\n",
    "try:\n",
    "    pipeline = pdal.Pipeline(json.dumps(dsm_pipeline))\n",
    "    pipeline.execute()\n",
    "    print(f\"DSM created successfully: {dsm_path}\")\n",
    "    \n",
    "    # Calculate CHM by subtracting DEM from DSM\n",
    "    with rasterio.open(dem_path) as dem_src, rasterio.open(dsm_path) as dsm_src:\n",
    "        dem_data = dem_src.read(1)\n",
    "        dsm_data = dsm_src.read(1)\n",
    "        \n",
    "        # Calculate CHM (DSM - DEM)\n",
    "        chm_data = dsm_data - dem_data\n",
    "        \n",
    "        # Set negative values to zero (sometimes happens due to interpolation artifacts)\n",
    "        chm_data[chm_data < 0] = 0\n",
    "        \n",
    "        # Write CHM to file\n",
    "        with rasterio.open(\n",
    "            chm_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=dem_src.height,\n",
    "            width=dem_src.width,\n",
    "            count=1,\n",
    "            dtype=chm_data.dtype,\n",
    "            crs=dem_src.crs,\n",
    "            transform=dem_src.transform,\n",
    "        ) as chm_dst:\n",
    "            chm_dst.write(chm_data, 1)\n",
    "        \n",
    "        print(f\"CHM created successfully: {chm_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating DSM or CHM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Terrain Models\n",
    "\n",
    "Let's visualize the DEM and CHM we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DEM\n",
    "try:\n",
    "    with rasterio.open(dem_path) as src:\n",
    "        dem_data = src.read(1, masked=True)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        im = ax.imshow(dem_data, cmap='terrain')\n",
    "        ax.set_title('Digital Elevation Model (DEM)')\n",
    "        plt.colorbar(im, ax=ax, label='Elevation (m)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error visualizing DEM: {e}\")\n",
    "\n",
    "# Visualize CHM\n",
    "try:\n",
    "    with rasterio.open(chm_path) as src:\n",
    "        chm_data = src.read(1, masked=True)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        im = ax.imshow(chm_data, cmap='viridis')\n",
    "        ax.set_title('Canopy Height Model (CHM)')\n",
    "        plt.colorbar(im, ax=ax, label='Height (m)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print some statistics about the canopy height\n",
    "        valid_data = chm_data[~chm_data.mask]\n",
    "        print(f\"Canopy Height Statistics:\")\n",
    "        print(f\"  Min: {np.min(valid_data):.2f} m\")\n",
    "        print(f\"  Max: {np.max(valid_data):.2f} m\")\n",
    "        print(f\"  Mean: {np.mean(valid_data):.2f} m\")\n",
    "        print(f\"  Median: {np.median(valid_data):.2f} m\")\n",
    "        print(f\"  Standard Deviation: {np.std(valid_data):.2f} m\")\n",
    "except Exception as e:\n",
    "    print(f\"Error visualizing CHM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Detection Exploration\n",
    "\n",
    "Now let's explore some basic feature detection techniques that could help identify archaeological features or significant ecological structures (like giant trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slope Analysis - can help identify artificial structures\n",
    "try:\n",
    "    with rasterio.open(dem_path) as src:\n",
    "        dem = src.read(1, masked=True)\n",
    "        transform = src.transform\n",
    "        \n",
    "        # Calculate x and y gradients\n",
    "        dx, dy = np.gradient(dem)\n",
    "        \n",
    "        # Calculate slope in degrees\n",
    "        slope = np.degrees(np.arctan(np.sqrt(dx**2 + dy**2)))\n",
    "        \n",
    "        # Calculate aspect (direction of slope)\n",
    "        aspect = np.degrees(np.arctan2(-dx, dy))\n",
    "        \n",
    "        # Visualize slope\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        im = ax.imshow(slope, cmap='YlOrRd')\n",
    "        ax.set_title('Slope (degrees)')\n",
    "        plt.colorbar(im, ax=ax, label='Slope (degrees)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Visualize aspect\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        im = ax.imshow(aspect, cmap='hsv')\n",
    "        ax.set_title('Aspect (degrees)')\n",
    "        plt.colorbar(im, ax=ax, label='Aspect (degrees)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating slope and aspect: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic tree detection using local maxima in the CHM\n",
    "try:\n",
    "    from skimage.feature import peak_local_max\n",
    "    from scipy import ndimage\n",
    "    \n",
    "    with rasterio.open(chm_path) as src:\n",
    "        chm = src.read(1, masked=True)\n",
    "        transform = src.transform\n",
    "        \n",
    "        # Fill masked values with 0\n",
    "        chm_filled = chm.filled(0)\n",
    "        \n",
    "        # Apply Gaussian filter to smooth the CHM\n",
    "        chm_smooth = ndimage.gaussian_filter(chm_filled, sigma=1)\n",
    "        \n",
    "        # Find local maxima (potential tree tops)\n",
    "        # Adjust min_distance and threshold based on expected tree spacing and minimum height\n",
    "        min_height = 5  # Minimum tree height in meters\n",
    "        min_distance = 5  # Minimum distance between trees in pixels\n",
    "        \n",
    "        # Find peaks\n",
    "        tree_tops = peak_local_max(\n",
    "            chm_smooth, \n",
    "            min_distance=min_distance,\n",
    "            threshold_abs=min_height,\n",
    "            indices=True\n",
    "        )\n",
    "        \n",
    "        # Get tree heights at peak locations\n",
    "        tree_heights = chm_smooth[tree_tops[:, 0], tree_tops[:, 1]]\n",
    "        \n",
    "        # Create a visualization\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        im = ax.imshow(chm_smooth, cmap='viridis')\n",
    "        ax.plot(tree_tops[:, 1], tree_tops[:, 0], 'r.', markersize=3)\n",
    "        ax.set_title(f'Tree Top Detection (n={len(tree_tops)})')\n",
    "        plt.colorbar(im, ax=ax, label='Height (m)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics about detected trees\n",
    "        print(f\"Tree Detection Results:\")\n",
    "        print(f\"  Number of trees detected: {len(tree_heights)}\")\n",
    "        print(f\"  Mean tree height: {np.mean(tree_heights):.2f} m\")\n",
    "        print(f\"  Max tree height: {np.max(tree_heights):.2f} m\")\n",
    "        print(f\"  Min tree height: {np.min(tree_heights):.2f} m\")\n",
    "        \n",
    "        # Find potential giant trees (e.g., top 5% by height)\n",
    "        height_threshold = np.percentile(tree_heights, 95)\n",
    "        giant_trees = tree_tops[tree_heights > height_threshold]\n",
    "        giant_tree_heights = tree_heights[tree_heights > height_threshold]\n",
    "        \n",
    "        print(f\"\\nPotential Giant Trees (height > {height_threshold:.2f} m):\")\n",
    "        print(f\"  Number detected: {len(giant_trees)}\")\n",
    "        if len(giant_trees) > 0:\n",
    "            print(f\"  Mean height: {np.mean(giant_tree_heights):.2f} m\")\n",
    "            print(f\"  Max height: {np.max(giant_tree_heights):.2f} m\")\n",
    "            \n",
    "            # Highlight giant trees in a new visualization\n",
    "            fig, ax = plt.subplots(figsize=(12, 10))\n",
    "            im = ax.imshow(chm_smooth, cmap='viridis')\n",
    "            ax.plot(tree_tops[:, 1], tree_tops[:, 0], 'r.', markersize=2, alpha=0.5)\n",
    "            ax.plot(giant_trees[:, 1], giant_trees[:, 0], 'yo', markersize=8, mfc='none')\n",
    "            ax.set_title(f'Potential Giant Trees (n={len(giant_trees)})')\n",
    "            plt.colorbar(im, ax=ax, label='Height (m)')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error in tree detection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've explored the basics of working with LiDAR data for the Amazon Deep Insights project. We've:\n",
    "\n",
    "1. Set up our environment and downloaded sample data\n",
    "2. Visualized LiDAR point clouds in 3D and profile views\n",
    "3. Generated Digital Elevation Models (DEM) and Canopy Height Models (CHM)\n",
    "4. Performed basic terrain analysis (slope, aspect)\n",
    "5. Explored simple tree detection techniques\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Data Acquisition**: Download actual Amazon rainforest LiDAR data from the sources listed in Links.md\n",
    "2. **Advanced Processing**: Implement more sophisticated filtering and classification techniques\n",
    "3. **Feature Detection**: Develop algorithms to detect potential archaeological features (geometric patterns, mounds)\n",
    "4. **RAG Integration**: Connect the processed data to our RAG system for natural language querying\n",
    "5. **Visualization**: Create interactive maps and dashboards for exploration\n",
    "\n",
    "The sample data used in this notebook is for demonstration purposes only. For the actual project, we'll need to work with the Amazon-specific datasets listed in the project resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
