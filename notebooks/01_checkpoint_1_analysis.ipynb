{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI-to-Z Challenge: Checkpoint 1 Analysis\n",
    "\n",
    "This notebook covers the analysis part of Checkpoint 1. We will:\n",
    "1. Load the DTM and LiDAR data downloaded in the previous step.\n",
    "2. Extract a raster chunk from the DTM.\n",
    "3. Call the OpenAI `o3` model (using `gpt-4o-mini` as a proxy) to identify potential archaeological features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import necessary libraries and load the OpenAI API key from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T03:45:07.149571Z",
     "iopub.status.busy": "2025-06-30T03:45:07.149290Z",
     "iopub.status.idle": "2025-06-30T03:45:07.463441Z",
     "shell.execute_reply": "2025-06-30T03:45:07.463202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and OpenAI client initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import laspy\n",
    "import numpy as np\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Load environment variables (ensure .env file has OPENAI_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "print(\"Libraries imported and OpenAI client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Downloaded Data\n",
    "\n",
    "Specify the paths to the downloaded DTM and LiDAR files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T03:45:07.478990Z",
     "iopub.status.busy": "2025-06-30T03:45:07.478741Z",
     "iopub.status.idle": "2025-06-30T03:45:07.483854Z",
     "shell.execute_reply": "2025-06-30T03:45:07.483652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load DTM from: /Users/shg/Projects/openai-a-z-challenge/data/raw/TAL_A01_2018/TAL_A01_2018_DTM/TAL01L0001C0001.grd\n",
      "Attempting to load LiDAR from: /Users/shg/Projects/openai-a-z-challenge/data/raw/TAL_A01_2018/TAL_A01_2018_LAS/TAL01L0001C0001.las\n",
      "DTM file loaded successfully.\n",
      "  - Bounds: BoundingBox(left=610108.0, bottom=8866715.0, right=611109.0, top=8867716.0)\n",
      "  - CRS: None\n",
      "\n",
      "LiDAR file loaded successfully.\n",
      "  - Bounds: (np.float64(610460.18), np.float64(8866715.790000001), np.float64(611108.6), np.float64(8867507.97))\n",
      "  - CRS: None\n"
     ]
    }
   ],
   "source": [
    "# Update these paths based on the extracted data\n",
    "dtm_raster_path = '/Users/shg/Projects/openai-a-z-challenge/data/raw/TAL_A01_2018/TAL_A01_2018_DTM/TAL01L0001C0001.grd'\n",
    "lidar_laz_path = '/Users/shg/Projects/openai-a-z-challenge/data/raw/TAL_A01_2018/TAL_A01_2018_LAS/TAL01L0001C0001.las'\n",
    "\n",
    "print(f\"Attempting to load DTM from: {dtm_raster_path}\")\n",
    "print(f\"Attempting to load LiDAR from: {lidar_laz_path}\")\n",
    "\n",
    "try:\n",
    "    # Load DTM raster data\n",
    "    with rasterio.open(dtm_raster_path) as src:\n",
    "        dtm_bounds = src.bounds\n",
    "        dtm_crs = src.crs\n",
    "        print(f\"DTM file loaded successfully.\")\n",
    "        print(f\"  - Bounds: {dtm_bounds}\")\n",
    "        print(f\"  - CRS: {dtm_crs}\")\n",
    "\n",
    "    # Load LiDAR point cloud data\n",
    "    with laspy.open(lidar_laz_path) as lidar_file:\n",
    "        lidar_header = lidar_file.header\n",
    "        lidar_bounds = (lidar_header.x_min, lidar_header.y_min, lidar_header.x_max, lidar_header.y_max)\n",
    "        print(f\"\\nLiDAR file loaded successfully.\")\n",
    "        print(f\"  - Bounds: {lidar_bounds}\")\n",
    "        print(f\"  - CRS: {lidar_header.parse_crs()}\")\n",
    "except (FileNotFoundError, rasterio.errors.RasterioIOError) as e:\n",
    "    print(f\"Error: {e}. Please ensure the file paths are correct and data was downloaded and extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Overlapping Area and Create Raster Chunk\n",
    "\n",
    "We'll read a chunk from the center of the DTM raster for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T03:45:07.484885Z",
     "iopub.status.busy": "2025-06-30T03:45:07.484818Z",
     "iopub.status.idle": "2025-06-30T03:45:07.501575Z",
     "shell.execute_reply": "2025-06-30T03:45:07.501370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted a raster chunk of shape: (3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "def get_raster_chunk(tif_path, window_size=(512, 512)):\n",
    "    \"\"\"Reads a chunk from the center of a raster file.\"\"\"\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        center_x = src.width // 2\n",
    "        center_y = src.height // 2\n",
    "\n",
    "        window = rasterio.windows.Window(\n",
    "            center_x - window_size[0] // 2,\n",
    "            center_y - window_size[1] // 2,\n",
    "            window_size[0],\n",
    "            window_size[1]\n",
    "        )\n",
    "\n",
    "        # Read single band from the DTM and replicate to 3 channels for visualization\n",
    "        band = src.read(1, window=window)\n",
    "        chunk = np.stack([band] * 3, axis=0)\n",
    "    return chunk\n",
    "\n",
    "try:\n",
    "    raster_chunk = get_raster_chunk(dtm_raster_path)\n",
    "    print(f\"Successfully extracted a raster chunk of shape: {raster_chunk.shape}\")\n",
    "except (NameError, FileNotFoundError, rasterio.errors.RasterioIOError) as e:\n",
    "    print(f\"Skipping chunk extraction due to previous error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for OpenAI Model\n",
    "\n",
    "The OpenAI Vision API accepts images. We'll convert our raster chunk (a NumPy array) into a base64-encoded PNG image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T03:45:07.502784Z",
     "iopub.status.busy": "2025-06-30T03:45:07.502718Z",
     "iopub.status.idle": "2025-06-30T03:45:07.649451Z",
     "shell.execute_reply": "2025-06-30T03:45:07.649206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raster chunk successfully converted to base64 encoded image.\n"
     ]
    }
   ],
   "source": [
    "def numpy_to_base64(np_array):\n",
    "    \"\"\"Converts a NumPy array to a base64 encoded PNG image.\"\"\"\n",
    "    # Normalize and convert to 8-bit integer\n",
    "    np_array = np.moveaxis(np_array, 0, -1) # Move channels to the last dimension\n",
    "    # Handle cases where min and max are the same to avoid division by zero\n",
    "    min_val, max_val = np_array.min(), np_array.max()\n",
    "    if max_val == min_val:\n",
    "        normalized = np.zeros_like(np_array)\n",
    "    else:\n",
    "        normalized = (np_array - min_val) / (max_val - min_val)\n",
    "    image_data = (normalized * 255).astype(np.uint8)\n",
    "\n",
    "    img = Image.fromarray(image_data, 'RGB')\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "try:\n",
    "    base64_image = numpy_to_base64(raster_chunk)\n",
    "    print(\"Raster chunk successfully converted to base64 encoded image.\")\n",
    "except NameError:\n",
    "    print(\"Skipping image conversion due to previous error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Call the OpenAI API\n",
    "\n",
    "Now we'll send the image to the `gpt-4o-mini` model with a specific prompt to look for archaeological features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T03:45:07.650830Z",
     "iopub.status.busy": "2025-06-30T03:45:07.650755Z",
     "iopub.status.idle": "2025-06-30T03:45:11.730418Z",
     "shell.execute_reply": "2025-06-30T03:45:11.729798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API call successful.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "PROMPT = \"\"\"\n",
    "Analyze this digital terrain model (DTM) image from the Amazon rainforest. Look for potential hidden archaeological features.\n",
    "Specifically, identify any geometric shapes like rectangles, circles, or long straight ditches that are 80 meters or more across.\n",
    "For each potential feature, describe its shape and provide its approximate center coordinates within the image (using a 0-1 scale for x and y, where 0,0 is the top-left corner).\n",
    "If no such features are visible, state that clearly.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": PROMPT},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.2, # Lower temperature for more deterministic output\n",
    "    )\n",
    "    print(\"API call successful.\")\n",
    "except NameError:\n",
    "    print(\"Skipping API call due to previous error.\")\n",
    "except openai.APIError as e:\n",
    "    print(f\"OpenAI API Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Log Model Response and ID\n",
    "\n",
    "For reproducibility, we'll print the model's full response and the exact model ID returned by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T03:45:11.732928Z",
     "iopub.status.busy": "2025-06-30T03:45:11.732721Z",
     "iopub.status.idle": "2025-06-30T03:45:11.736482Z",
     "shell.execute_reply": "2025-06-30T03:45:11.735738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MODEL RESPONSE ---\n",
      "I'm unable to analyze images directly. However, if you're looking for potential archaeological features in a digital terrain model (DTM) of the Amazon rainforest, consider using specialized software or tools designed for remote sensing and terrain analysis. Look for geometric patterns, variations in elevation, or anomalies that could indicate human-made structures. If you have specific features in mind, I can help guide you on how to identify them or what to look for!\n",
      "\n",
      "--- REPRODUCIBILITY INFO ---\n",
      "Model Used: gpt-4o-mini-2024-07-18\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_response_content = response.choices[0].message.content\n",
    "    model_id_used = response.model\n",
    "\n",
    "    print(\"--- MODEL RESPONSE ---\")\n",
    "    print(model_response_content)\n",
    "    print(\"\\n--- REPRODUCIBILITY INFO ---\")\n",
    "    print(f\"Model Used: {model_id_used}\")\n",
    "except NameError:\n",
    "    print(\"No response to log.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
