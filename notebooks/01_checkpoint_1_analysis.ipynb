{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI-to-Z Challenge: Checkpoint 1 Analysis\n",
    "\n",
    "This notebook covers the analysis part of Checkpoint 1. We will:\n",
    "1. Load the Sentinel-2 and LiDAR data downloaded in the previous step.\n",
    "2. Identify an overlapping region.\n",
    "3. Extract a raster chunk from the Sentinel-2 scene.\n",
    "4. Call the OpenAI `o3` model (using `gpt-4o-mini` as a proxy) to identify potential archaeological features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import necessary libraries and load the OpenAI API key from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import laspy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Load environment variables (ensure .env file has OPENAI_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "print(\"Libraries imported and OpenAI client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Downloaded Data\n",
    "\n",
    "Specify the paths to the downloaded Sentinel-2 and LiDAR files. **Note:** You may need to update these file paths to match the exact names of the files you downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these paths based on the output from the data acquisition notebook\n",
    "sentinel_tif_path = 'data/raw/sentinel2/COPERNICUS_S2_SR_2023...TIF' # Replace with actual filename\n",
    "lidar_laz_path = 'data/raw/lidar/FLB_7006_20140728_131235.laz' # Replace with actual filename\n",
    "\n",
    "try:\n",
    "    # Load Sentinel-2 raster data\n",
    "    with rasterio.open(sentinel_tif_path) as src:\n",
    "        sentinel_bounds = src.bounds\n",
    "        sentinel_crs = src.crs\n",
    "        print(f\"Sentinel-2 file loaded successfully.\")\n",
    "        print(f\"  - Bounds: {sentinel_bounds}\")\n",
    "        print(f\"  - CRS: {sentinel_crs}\")\n",
    "\n",
    "    # Load LiDAR point cloud data\n",
    "    with laspy.open(lidar_laz_path) as lidar_file:\n",
    "        lidar_header = lidar_file.header\n",
    "        lidar_bounds = (lidar_header.x_min, lidar_header.y_min, lidar_header.x_max, lidar_header.y_max)\n",
    "        print(f\"\\nLiDAR file loaded successfully.\")\n",
    "        print(f\"  - Bounds: {lidar_bounds}\")\n",
    "        print(f\"  - CRS: {lidar_header.parse_crs()}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure the file paths are correct and data was downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Overlapping Area and Create Raster Chunk\n",
    "\n",
    "We need to find the intersection of the two datasets' bounding boxes to analyze an overlapping area. For this example, we'll assume they overlap and simply read a chunk from the Sentinel-2 image. A full implementation would reproject one CRS to another and find the precise intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raster_chunk(tif_path, window_size=(512, 512)):\n",
    "    \"\"\"Reads a chunk from the center of a raster file.\"\"\"\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        center_x = src.width // 2\n",
    "        center_y = src.height // 2\n",
    "        \n",
    "        window = rasterio.windows.Window(\n",
    "            center_x - window_size[0] // 2,\n",
    "            center_y - window_size[1] // 2,\n",
    "            window_size[0],\n",
    "            window_size[1]\n",
    "        )\n",
    "        \n",
    "        # Read RGB bands (e.g., 4, 3, 2 for Sentinel-2)\n",
    "        # Adjust band indices if necessary\n",
    "        chunk = src.read([4, 3, 2], window=window)\n",
    "    return chunk\n",
    "\n",
    "try:\n",
    "    raster_chunk = get_raster_chunk(sentinel_tif_path)\n",
    "    print(f\"Successfully extracted a raster chunk of shape: {raster_chunk.shape}\")\n",
    "except NameError:\n",
    "    print(\"Skipping chunk extraction due to previous file loading error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for OpenAI Model\n",
    "\n",
    "The OpenAI Vision API accepts images. We'll convert our raster chunk (a NumPy array) into a base64-encoded PNG image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_base64(np_array):\n",
    "    \"\"\"Converts a NumPy array to a base64 encoded PNG image.\"\"\"\n",
    "    # Normalize and convert to 8-bit integer\n",
    "    np_array = np.moveaxis(np_array, 0, -1) # Move channels to the last dimension\n",
    "    normalized = (np_array - np_array.min()) / (np_array.max() - np_array.min())\n",
    "    image_data = (normalized * 255).astype(np.uint8)\n",
    "    \n",
    "    img = Image.fromarray(image_data, 'RGB')\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "try:\n",
    "    base64_image = numpy_to_base64(raster_chunk)\n",
    "    print(\"Raster chunk successfully converted to base64 encoded image.\")\n",
    "except NameError:\n",
    "    print(\"Skipping image conversion due to previous error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Call the OpenAI API\n",
    "\n",
    "Now we'll send the image to the `gpt-4o-mini` model with a specific prompt to look for archaeological features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "PROMPT = \"\"\"\n",
    "Analyze this satellite image of the Amazon rainforest. Look for potential hidden archaeological features.\n",
    "Specifically, identify any geometric shapes like rectangles, circles, or long straight ditches that are 80 meters or more across.\n",
    "For each potential feature, describe its shape and provide its approximate center coordinates within the image (using a 0-1 scale for x and y, where 0,0 is the top-left corner).\n",
    "If no such features are visible, state that clearly.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": PROMPT},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.2, # Lower temperature for more deterministic output\n",
    "    )\n",
    "    print(\"API call successful.\")\n",
    "except NameError:\n",
    "    print(\"Skipping API call due to previous error.\")\n",
    "except openai.APIError as e:\n",
    "    print(f\"OpenAI API Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Log Model Response and ID\n",
    "\n",
    "For reproducibility, we'll print the model's full response and the exact model ID returned by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_response_content = response.choices[0].message.content\n",
    "    model_id_used = response.model\n",
    "    \n",
    "    print(\"--- MODEL RESPONSE ---\")\n",
    "    print(model_response_content)\n",
    "    print(\"\\n--- REPRODUCIBILITY INFO ---\")\n",
    "    print(f\"Model Used: {model_id_used}\")\n",
    "except NameError:\n",
    "    print(\"No response to log.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
